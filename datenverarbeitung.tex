\chapter{Datenverarbeitung}
\label{ch:data_processing}

\section{Klassisches Analysevorgehen}
\label{sec:common_analysis_approach_part2}

In Kapitel \ref{sec:common_analysis_approach_part1} wird beschrieben, wie bei der Open-Source Software \textit{Autopsy} ein Fall angelegt und Datenträgerabbilder hinzugefügt werden können. Schon beim Importieren einer Datenquelle können diverse Module zur automatisierten Datenaufbereitung aktiviert werden. Dies entspricht der dritten Phase, dem \textit{Sichten und Aufbereiten}, aus dem allgemeinen forensischen Analyseprozess.\footnote{Siehe Abbildung \ref{fig:digital_forensics_process} in Kapitel \ref{sec:common_analysis_process}.} Diese Module werden nachfolgend beschrieben, um einen Vergleich zur hier entwickelten forensischen Analyseplattform zu ermöglichen.
Abbildung \ref{fig:autopsy_2_ingest_modules} zeigt die verfügbaren Module.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.78\textwidth]{./resource/autopsy_2_ingest_modules.png}
  \caption{Module zur automatischen Datenverarbeitung bei Autopsy}
  \label{fig:autopsy_2_ingest_modules}
\end{figure}

\noindent
Nachfolgend werden die Module kurz beschrieben und mit der Funktionalität der hier entwickelten forensischen Analyseplattform verglichen. Zuerst werden die Module aufgelistet, welche ganz oder teilweise in der forensischen Analyseplattform implementiert sind.\footnote{Siehe Link: \url{http://sleuthkit.org/autopsy/docs/user-docs/4.3/}. Letzter Zugriff: 09.08.2018.}

\begin{itemize}
\item Das \textit{File Type Identification Modul} ermöglicht die Ermittlung von Medientypen und Dateiformaten, wie beispielsweise \textit{JPEG}-Bilder. Autopsy nutzt hier die Open-Source Bibliothek Apache Tika\texttrademark. Eine gleichwertige Funktionalität ist auch in der forensischen Analyseplattform vorhanden.\footnote{Siehe Kapitel \ref{subsec:media_types}.}
\item Das \textit{Correlation Engine Modul} bietet die Möglichkeit, mehrere Fälle und ihre Datenquellen miteinander zu verknüpfen, um Beziehungen untereinander zu erkennen. Analog hierzu bietet auch die forensische Analyseplattform, mit dem Auffinden von Datei-Duplikaten, eine einfache Funktion, um Beziehungen zwischen mehreren Fällen und deren Asservate herzustellen.\footnote{Siehe Kapitel \ref{subsec:duplicate_files}.}
\item Das \textit{Keyword Search Modul} enthält eine Schlüsselwortsuche. Hierzu werden Dateien mithilfe von Apache Solr indiziert. Darüber hinaus kann, basierend auf regulären Ausdrücken, nach bestimmten Strukturen, wie beispielsweise E-Mail- oder Web-Adressen, gesucht werden. Die hier entwickelte Analyseplattform implementiert auch eine Textsuche auf Basis von Apache Solr. Derzeit ist es allerdings noch nicht möglich, ganze Dateiinhalte zu indexieren.\footnote{Siehe Kapitel \ref{subsec:file_indexing}.}
\end{itemize}

\noindent
Nachfolgend werden die Module von Autopsy aufgelistet, welche noch nicht in der forensischen Analyseplattform vorhanden sind und in Weiterentwicklungen der Plattform implementiert werden können.
\begin{itemize}
\item Das \textit{Recent Activity Modul} ermittelt anhand der Zeitstempel alle interessanten Tätigkeiten der letzten 7 Tage, wie zum Beispiel besuchte Websites von diversen Browsern.
\item Das \textit{Hash Lookup Modul} vergleicht die Datei-Hashsummen mit bereits bekannten Hashsummen. Damit können bekannte Betriebssystemdateien schnell gefiltert werden. Analog zu einem Virenscanner kann dadurch auch bekannte bösartige Malware gefunden werden.
\item Das \textit{Embedded File Extractor Modul} entpackt Dateiarchive, wie beispielsweise \textit{ZIP}-Dateien.
\item Das \textit{Exif Parser Modul} wertet die sogenannten \textit{EXIF}-Metadaten aus, welche in \textit{JPEG}-Bilddateien enthalten sind. In der forensischen Analyseplattform kann diese Funktionalität auch mit Apache Tika implementiert werden.
\item Das \textit{Email Parser Modul} extrahiert E-Mails aus diversen E-Mail Archiven, wie beispielsweise dem \textit{\acrshort{pst}}-Dateiformat, welches von Microsoft Outlook genutzt wird.
\item Das \textit{Extension Mismatch Detector Modul} ermittelt Dateien, deren Dateiendung nicht zum analysierten Medientyp des Dateiinhalts passen. Dieser Unterschied kann auf versteckte Dateien hinweisen. Eine ähnliche Funktion kann in der forensischen Analyseplattform über die Volltextsuche implementiert werden, da die Dateiendung und der Medientyp bekannt sind.
\item Das \textit{E01 Verifier Modul} verifiziert, speziell für das forensische Analyseformat \textit{E01}, die Dateiprüfsummen mit den gespeicherten Prüfsummen. Damit können Änderungen an einem sichergestellten Asservat identifiziert werden.
\item Das \textit{Interesting Files Identifier Modul} identifiziert interessante Dateien, welche vorher durch bestimmte Suchkriterien, wie beispielsweise Dateinamen und Datentypen, definiert wurden.
\item Das \textit{PhotoRec Carver Modul} ermöglicht das bereits beschriebene File Carving auf ungenutzten Speicherbereichen. Diese Funktionalität wird nicht von der forensischen Analyseplattform unterstützt, da ungenutzte Speicherbereiche aus den originalen Datenträgerabbildern nicht in das Analysesystem importiert werden.
\item Das \textit{Encryption Detection Modul} ermöglicht das Auffinden von verschlüsselten Containern. Hierbei wird nach Dateien gesucht, deren Inhalt eine hohe Entropie aufweisen.\footnote{In der Informationstechnik beschreibt die Entropie ein Maß zur Bestimmung des mittleren Informationsgehalts pro Zeichen. 
%Bei verschlüsselten Containern ist diese Entropie sehr hoch verglichen zu unverschlüsselten Dateien.
}
\item Das \textit{Virtual Machine Extractor Modul} kann Dateien aus Systemabbildern von virtuellen Maschinen extrahieren, welche im Asservat gespeichert sind.
\item Das \textit{Android Analyzer Modul} ermöglicht das Extrahieren von bestimmten Informationen aus spezifischen Dateien des Android-Betriebssystems. So können beispielsweise Kontaktdaten und Nachrichten aus spezifischen SQLite-Datenbanken ermittelt werden.
\end{itemize}

\noindent
Nachdem die benötigten Module in Autopsy aktiviert und nach fallspezifischen Eigenschaften konfiguriert wurden, erfolgt die Datenaufbereitung im Hintergrund. Parallel hierzu kann der Analyst bereits die Rohdaten analysieren.\footnote{Siehe Kapitel \ref{ch:data_visualization} zur Visualisierung von Daten.}\\

\section{Umsetzung in der forensischen Analyseplattform}

Zur Datenverarbeitung in der forensischen Analyseplattform wird Apache Spark\texttrademark\thinspace genutzt. Der physikalische Aufbau wird im Grundlagenkapitel \ref{sec:theory_spark} behandelt. In diesem Kapitel wird die Implementierung der Datenverarbeitung betrachtet.\\ 

\noindent
Bei Apache Spark (Version 2.3.0) gibt es diverse Programmierschnittstellen (\acrshort{api}s), wie Daten geladen werden können. Es besteht die Möglichkeit, Daten mithilfe von \textit{\glspl{rdd}} zu laden und zu verarbeiten. Aufbauend auf diesen RDDs können die Daten konvertiert, gefiltert oder aggregiert werden.
Listing \ref{lst:spark_rdd_word_count} zeigt ein simples Beispiel zur Nutzung dieser RDDs, um alle Wörter aus einer Beispieldatei \path{history.txt} und deren Vorkommen aufzulisten. Die Implementierung kann, abhängig von der Dateigröße und der Konfiguration von Spark, verteilt auf mehreren Knoten ausgeführt werden, um die Datenverarbeitung zu beschleunigen. Die Daten werden aus dem HDFS gelesen und dort gespeichert.

\clearpage

\begin{lstlisting}[label={lst:spark_rdd_word_count},caption= Beispielimplementierung eines Spark RDDs ,captionpos=b,frame=single,style=customjava]
//Datei aus dem HDFS lesen
JavaRDD<String> textFile = sc.textFile("hdfs://data/history.txt");
JavaPairRDD<String, Integer> counts = textFile
    // Text separieren
    .flatMap(s -> Arrays.asList(s.split(" ")).iterator())
    // Wortvorkommen ermitteln
    .mapToPair(word -> new Tuple2<>(word, 1))
    .reduceByKey((a, b) -> a + b);
// Ergebnis in HDFS speichern
counts.saveAsTextFile("hdfs://...");
\end{lstlisting}


\noindent
Bei der Weiterentwicklung von Spark sind in neueren Versionen sogenannte \textit{DataFrames} und \textit{DataSets} hinzugekommen. Diese Datenstrukturen beschreiben eine Schnittstelle, welche der Sichtweise von Tabellen ähnelt. Die Implementierung dieser Typen nutzt wiederum die bereits beschriebenen RDDs.\\
Welche Datenstrukturen für die Anwendungsfälle von der forensischen Analyseplattform genutzt werden sollten, hängt primär von der Art der Daten ab.
DataFrames und DataSets sind optimiert für strukturierte und semi-strukturierte Daten. Diese Daten lassen sich beispielsweise in Tabellenstrukturen einlesen und verarbeiten. Es gibt komplexe Operationen auf diesen Tabellen, welche dem klassischen SQL Syntax sehr nahe kommen. Apache Spark kann bei der Nutzung von DataFrames und DataSets viele Optimierungen bei der Ausführung und Verarbeitung durchführen. Andererseits sind diese Strukturen ungeeignet bei unstrukturierten Daten, wie beispielsweise Multimediadateien und beliebigen Dateien im Allgemeinen.\cite[S. 66 ff.]{data_processing_spark2}\\

\noindent
Wie beim Datenimport schon beschrieben, sind die Metadaten der analysierten Datenträger strukturiert, beziehungsweise semi-strukturiert, in HBase abgespeichert. Prinzipiell wäre es also möglich, auch mit DataSets und DataFrames auf diese Daten zuzugreifen. Letztlich kommt es aber auf die Anbindung zwischen Apache Spark und Apache HBase an. Es existieren zwei unterschiedliche Connectoren.\footnote{Bei Apache Spark sind Connectoren eine Art von Bibliotheken, welche es ermöglichen im Ausführungskontext auf andere Systeme, wie beispielsweise Datenbanken oder Dateisysteme, zuzugreifen.} Der \textit{Hortonworks SHC} Connector ermöglicht die Interaktion mit Daten in HBase und nutzt dafür die DataFrame/DataSet Datenstrukturen.\footnote{Siehe Link: \url{https://github.com/hortonworks-spark/shc}. Letzter Zugriff: 15.8.2018.}\\ 
Als Pendant existiert ein weiterer \textit{HBase-Spark Connector} auf Basis von RDDs. Letzterer wird im Rahmen dieser Thesis genutzt, um Daten von HBase zu lesen und zu schreiben.\footnote{Siehe Link: \url{https://github.com/apache/hbase/tree/master/hbase-spark} (Letzter Zugriff: 15.8.2018) und dessen Nutzung im Projekt \textit{foam-processing-spark} unter dem Link: \url{https://github.com/jobusam/foam-processing-spark} (Letzter Zugriff: 25.9.2018).}
Daher wird die Datenverarbeitung mithilfe von Spark RDDs implementiert. Auch beim Auslesen von großen Dateien aus dem HDFS, werden Spark RDDs verwendet, da Spark eine passende Schnittstelle zum HDFS auf Basis von RDDs anbietet.

%TODO Schreiben über getrennte Verarbeitung von großen und kleinen Dateien, da die Schnittstelle nicht einheitlich ist (siehe ByteBuffer bei HBase und PortableDataStreams beim HDFS)

\section{Anwendungsfälle der Datenverarbeitung}
\subsection{Berechnung von Hashsummen}
Eine grundlegende Funktionalität einer forensischen Analysesoftware ist die Berechnung von Hashsummen, um die Integrität fallrelevanter Dateien für zukünftige Analysen prüfen zu können. Darüber hinaus können auf Basis der Hashsummen sehr einfach Duplikate erkannt werden (siehe Kapitel \ref{subsec:duplicate_files}).\\
In der derzeitigen Implementierung wird der \textit{\acrshort{sha}-512} Algorithmus genutzt, um eindeutige und kryptografisch sichere Hashsummen zu berechnen. Bei Autopsy hingegen werden \textit{\acrshort{md5}}-Hashsummen berechnet. Der MD5-Algorithmus gilt mittlerweile jedoch als kryptografisch unsicher, da Hash-Kollisionen in wenigen Stunden berechnet werden können.\cite[S. 240-243]{hacking_and_security}\\

\noindent
Abbildung \ref{fig:data_processing_hashes} verdeutlicht den Datenfluss bei der Ermittlung von Hashsummen.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.78\textwidth]{./resource/spark_processing_hashes.png}
  \caption{Datenfluss bei der Berechnung von Hashsummen}
  \label{fig:data_processing_hashes}
\end{figure}

\noindent
Die Datenverarbeitung von kleinen Dateien aus HBase und großen Dateien aus dem HDFS wird unabhängig voneinander in eigenen Spark-RDDs ausgeführt. Aufgrund der unterschiedlichen Datenquellen existieren auch zwei unterschiedliche Datentypen zum Auslesen der Dateiinhalte. Daher wird die Prozessierung in getrennten Tasks innerhalb der Spark Executoren ausgeführt. Um dann später die Hashsumme einer großen Datei aus dem HDFS in dem dazugehörigen Eintrag in HBase abzuspeichern, ist es notwendig den Zeilenschlüssel des Eintrags in HBase als Dateiname im HDFS zu verwenden. Dadurch können die Dateien aus dem HDFS einem Eintrag in HBase zugeordnet werden. Der ermittelte Hashwert wird darauf in der Spalte \textit{metadata:fileHash} in HBase  gespeichert.\footnote{Siehe auch HBase-Datenmodell in Abbildung \ref{fig:hbase_data_model}.}

\subsection{Erkennung von Duplikaten}
\label{subsec:duplicate_files}
Aufbauend auf der Berechnung von Hashsummen, können nun auch Datei-Duplikate ermittelt werden. Dadurch ist es möglich, Beziehungen zwischen mehreren Asservaten herzustellen, wenn gleiche Dateien gefunden werden.
Derzeit werden diese Ergebnisse aber nur als Text im HDFS abgespeichert. In einer Weiterentwicklung könnten diese Informationen sehr gut in einer Graphendatenbank, wie beispielsweise \textit{Neo4j}, abgespeichert werden. Hierdurch wäre visuell sehr schnell sichtbar, welche Beziehungen zwischen den Asservaten bestehen.\\ 
In diesem Kontext ist es auch sinnvoll, zwischen technischen Betriebssystemdateien und Nutzerdateien zu unterscheiden. Denn wenn zwei Asservate das gleiche Betriebssystem enthalten, existieren auch zugleich tausende Datei-Duplikate. Gerade für Nutzerdaten, wie beispielsweise Bilder, Dokumente und Videos, ist die Erkennung von Duplikaten interessant. Daran könnte später ermittelt werden, wie beispielsweise urheberrechtlich geschütztes Material über mehrere Systeme verbreitet wurde.\\

\noindent
Ein anderer Aspekt, welcher in einer Weiterentwicklung der Plattform implementiert werden könnte, ist der Vergleich mit bekannten Hashsummen von existierenden Datensätzen. Dies wird bei Autopsy verwendet, um gegebenenfalls Malware zu erkennen oder automatisiert nach spezifischen Dateien zu suchen. Ein Beispiel hierzu ist die sogenannte \textit{National Software Reference Library}, welche Hashsummen von bekannten Betriebssystem- und Anwendungsdateien enthält. Diese haben oftmals keinen forensischen Analysewert und können aus der Datenverarbeitung gefiltert werden.\footnote{Siehe Link: \url{https://www.nist.gov/itl/ssd/software-quality-group/nsrl-download/current-rds-hash-sets}. Letzter Zugriff: 9.9.2018.} \cite[S. 36]{digital_forensics}

\subsection{Ermittlung von Medientypen}
\label{subsec:media_types}

Das Erkennen von Medientypen ist ein elementarer Bestandteil bei der Datenanalyse. Autopsy nutzt hierbei die Open-Source Bibliothek Apache Tika\texttrademark. Dieses Projekt wird auch in der forensischen Analyseplattform genutzt und ermittelt die Dateitypen anhand des Dateiinhalts und der gegebenen Dateiendung.\\
Abbildung \ref{fig:data_processing_media_types} zeigt den Datenfluss bei der Ermittlung der Medientypen.\\

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.78\textwidth]{./resource/spark_processing_media_types.png}
  \caption{Datenfluss bei der Ermittlung von Medientypen}
  \label{fig:data_processing_media_types}
\end{figure}

\noindent
Prinzipiell kann Apache Tika den Medientyp auch nur anhand der Dateiendung oder anhand des Dateiinhalts erkennen. Wird allerdings nur die Dateiendung angegeben, so können beispielsweise Bilddateien mit gefälschter Dateiendung nicht korrekt erkannt werden. Wird hingegen nur der Dateiinhalt angegeben, so prüft Tika formatspezifische Bytesequenzen, welche oftmals am Dateianfang stehen. Bei Textdateien hingegen kann es nur identifizieren, dass es sich um eine Textdatei handeln könnte. Gerade bei Textdateien ist es sinnvoll, die Dateiendung in die Analyse mit aufzunehmen, um ein genaueres Ergebnis zu erhalten. Dadurch kann Tika beispielsweise ermitteln, ob es sich um eine \acrshort{csv}-Datei oder eine \acrshort{html}-Datei handelt. Allerdings sollten diese Ergebnisse kritisch hinterfragt werden. Wie bereits erwähnt, könnte eine gefälschte Dateiendung bei Textdateien zu einem falschen Ergebnis führen.\\
Das Ermitteln der Medientypen ist für die Datenvisualisierung sehr nützlich, da damit nach allen vorhandenen Bildern oder Videos gesucht werden kann.\footnote{Siehe Kapitel \ref{ch:data_visualization}.}

\subsection{Volltextsuche und Datenindexierung}
\label{subsec:file_indexing}
Um auf die Daten in der forensischen Analyseplattform einfach zugreifen zu können, wäre eine Volltextsuche sinnvoll. Diese sollte performant arbeiten, damit Ergebnisse zu Suchanfragen von beliebigen Schlüsselwörtern in Realzeit angezeigt werden können. Um dies zu ermöglichen, können die Dateiinhalte und Metadaten indexiert werden.\\

\noindent 
Es gibt hierzu zwei Open-Source Projekte, die eine Datenindexierung anbieten. Dies sind \textit{Apache Solr\texttrademark} und \textit{Elasticsearch\texttrademark}. Beide bieten eine Indexierung auf Basis von Apache Lucene an. Apache Solr und Apache Lucene werden im Grundlagenkapitel \ref{sec:theory_solr} beschrieben. Solr und Elasticsearch bieten einen ähnlichen Funktionsumfang. Im Rahmen dieser Thesis wird jedoch Apache Solr aus nachfolgenden Gründen genutzt.\\
Apache Solr bietet einen Cloud-Modus an und nutzt Apache ZooKeeper, um die einzelnen Instanzen zu koordinieren. Für Apache Solr existiert bereits eine Integration in das Hadoop-Framework und die hier verwendete Hortonworks Data Platform.\footnote{Siehe Link: \url{https://docs.hortonworks.com/HDPDocuments/HDPS/HDPS-3.0.0/bk_solr-search-installation/content/ch_hdp-search.html}. Letzter Zugriff: 9.9.2018.}\\
Elasticsearch ist auch horizontal skalierbar und kann auf einem Computer-Cluster installiert werden. Allerdings wird ein eigener Mechanismus zur Koordination des Verbunds genutzt und es existiert für die Hortonworks Data Platform kein Installationspaket zur Nutzung von Elasticsearch im Hadoop-Umfeld. Dies bedeutet, dass Elasticsearch eigenständig im Computer-Cluster ausgerollt und überwacht werden muss.\\

\noindent
Ein weiterer Punkt ist die Absicherung des Clusters mithilfe von Kerberos. Prinzipiell können beide Projekte mit Kerberos abgesichert werden. Allerdings ist die Funktionalität bei Elasticsearch nur in einem kostenpflichtigen Zusatzpaket der Firma \textit{Elastic}\footnote{Siehe Link: \url{https://www.elastic.co/}. Letzter Zugriff: 9.9.2018.} enthalten.\\

\noindent
Zuletzt unterscheiden sich beide Projekte auch in der Art und Weise, wie die Daten verteilt indexiert werden können. Bei Elasticsearch gibt es eine Bibliothek, welche die bereits bekannten \textit{Apache Spark Resilient Distributed Datasets} (RDDs) direkt indexieren kann.\footnote{Siehe Link: \url{https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html}. Letzter Zugriff: 9.9.2018.} Diese Variante eignet sich sehr gut zur Indexierung von Daten.\\
Apache Solr ermöglicht es auch, Daten mit Apache Spark zu indexieren.\footnote{Siehe Link: \url{https://github.com/lucidworks/spark-solr}. Letzter Zugriff: 9.9.2018.} Allerdings ist die Schnittstelle etwas umständlicher nutzbar, da nicht die RDDs, sondern die bereits beschriebenen DataFrames, verwendet werden.\\
%TODO Spark-Connector von Databricks?

\noindent
Ein weiterer Vorteil von Apache Solr ist die sogenannte \textit{\gls{solrcell}}, welche unter anderem auch Apache Tika nutzt, um Text aus PDF-Dokumenten, Word-Dokumenten oder Bildern zu extrahieren.\footnote{Siehe Link: \url{https://lucene.apache.org/solr/guide/7_4/uploading-data-with-solr-cell-using-apache-tika.html\#uploading-data-with-solr-cell-using-apache-tika}. Letzter Zugriff: 11.9.2018.} Diese Funktionalität kann wiederverwendet werden, um beliebige Dateiinhalte automatisch zu indexieren. Eine ähnliche Funktionalität für Elasticsearch müsste selbst implementiert werden.\\

\noindent
Eine interessante Möglichkeit, Dateien in Solr zu indexieren, bietet das Open-Source Projekt \textit{Lily HBase Indexer}.\footnote{Siehe Link: \url{https://github.com/NGDATA/hbase-indexer}. Letzter Zugriff: 9.9.2018.} Damit können Daten direkt aus HBase in den Solr-Cluster indexiert werden. 
Im Rahmen der Thesis wird das Projekt genutzt, um die Metadaten aus HBase in Solr zu indexieren. Abbildung \ref{fig:hbase_solr_indexing} stellt die physikalische Aufteilung mit dem Lily HBase Indexer dar.\\
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{./resource/hbase_solr_indexierung.pdf}
  \caption{Indexierung von Daten aus HBase in Solr}
  \label{fig:hbase_solr_indexing}
\end{figure}

\noindent
Das Projekt bietet mehrere Möglichkeiten, wie Daten indexiert werden können. Die hier genutzt Variante basiert auf einem sogenannten \textit{Side-Effect Processor}.\cite{hbase_sep} 
Dieser Side-Effect Processor registriert sich bei HBase als weiterer Knoten zur Datenreplikation.
Die Replikation läuft asynchron im Hintergrund und beeinflusst nicht die Schreibgeschwindigkeit von HBase. Jede neue Zeile und jede Datenmodifikation in HBase wird automatisch repliziert und dadurch in Solr indexiert.
Auch das Löschen von Zeilen wird entsprechend in Solr übernommen. Damit HBase die Daten repliziert, müssen die Tabellen und deren Spaltenfamilien extra zur Replikation markiert werden. Unabhängig von dieser Replikation, werden die Daten auf mehreren Knoten redundant gespeichert, da HBase die Informationen in Dateien auf dem HDFS ablegt.\\

\noindent
Ein Nachteil an dieser Variante ist die Tatsache, dass keine Binärdaten mithilfe von \textit{Solr Cell} verteilt indexiert werden können. Hierzu müsste der Lily HBase Indexer erweitert werden.\\
Ein weiteres Hindernis zeigt sich bei der Software-Installation. So kann zwar Apache Solr für das Computer-Cluster nachinstalliert werden. Allerdings wird Solr nur auf einem Knoten installiert. Bisher wurde noch keine Möglichkeit gefunden, wie Solr auf allen Knoten im Cluster ausgerollt werden kann. Dies sollte jedoch technisch möglich sein.\\
Für den Lily HBase Indexer wird eine eigene kleine Server-Komponente benötigt. Diese muss auf allen Knoten installiert sein, damit die Daten nach dem Prinzip der Datenlokalität ordnungsgemäß indexiert werden können. Allerdings läuft auch diese Server-Komponente derzeit nur auf einem Knoten.\\
Darüber hinaus wird der Lily HBase Indexer aktuell nicht weiterentwickelt.\footnote{Die letzte Änderung im Git-Repository erfolgte im Dezember 2016.} Basierend auf einigen Tests, ist die Integration des Indexers nur mit HBase in der Version 1.X.X möglich. Eine erfolgreiche Integration mit bereits verfügbaren neueren Versionen von HBase (ab Version 2.X.X) ist nicht realisierbar.\\ 

\noindent
Aufgrund dieser Probleme ist es sinnvoll, einen neuen Ansatz zur Datenindexierung zu suchen oder den bisherigen Indexer selbst weiterzuentwickeln. An dieser Stelle sollte das \textit{Spark-Solr} Projekt nochmals näher betrachtet werden.\footnote{Siehe Link: \url{https://github.com/lucidworks/spark-solr}. Letzter Zugriff: 9.9.2018.} Diese Implementierung könnte eventuell die bestehenden Probleme lösen. 



\section{Praxisbeispiele und deren Optimierungen}
Bei der Verarbeitung großer Datenmengen sollte immer das Prinzip der Datenlokalität berücksichtigt werden. Darüber hinaus gibt es weitere Optimierungen, um die Datenmenge auf das nötige Minimum zu reduzieren und die verfügbaren Ressourcen optimal auszunutzen. Im Hadoop-Umfeld und bei der Entwicklung mit Apache Spark ist es besonders wichtig, zu verstehen, auf welchen Knoten welche Teile des Programmcodes ausgeführt werden. Zu dieser Problematik werden in diesem Kapitel einige Beispiele herausgegriffen, welche während der Bearbeitung der Thesis aufgetreten sind.

\subsection*{Datenminimierung}
Die Metadaten aus HBase werden mithilfe des \textit{HBase-Spark Connectors} geladen. Dabei können die einzelnen Zeilen mit allen Spalten gelesen und in Spark verarbeitet werden. In Spark werden diese \textit{RDDs} nach bestimmten Kriterien gefiltert. Beispielsweise werden bei der Berechnung von Hashsummen die Zeilen nach logischen Dateien gefiltert. Verzeichnisse oder symbolische Links werden verworfen. 
Von den Dateien wird nur der Dateiinhalt benötigt. Weitere Metadaten sind nicht notwendig. Auch diese Werte können in den Spark RDDs einfach herausgefiltert werden. Allerdings befinden sich diese Daten zu dem Zeitpunkt schon in den einzelnen Spark Executoren. Viel effizienter ist es, diese Filterungen schon bei der Anfrage in der Datenbank durchzuführen. So bietet der HBase-Spark Connector die Möglichkeit, diverse Filter auf Zeilen und Spalten anzuwenden, um bereits in den einzelnen Region Server die benötigte Datenmenge auf das notwendige Minimum zu begrenzen. Diese Optimierung fällt bei kleinen Datenmengen in einer Testumgebung kaum ins Gewicht. Bei großen Datenmengen in einem produktiven Cluster können dadurch Ressourcen und Zeit eingespart werden.

\subsection*{Lazy Loading}
Ein weiterer Aspekt, um den Ressourcenverbrauch minimal zu halten, ist das sogenannte \textit{Lazy Loading}. Bei diesem Entwurfsmuster aus der Softwareentwicklung geht es darum, die Daten nur dann zu laden und zu verarbeiten, wenn sie auch wirklich benötigt werden. Apache Spark implementiert dieses Entwurfsmuster in den Spark RDDs.\cite{spark_rdd}\\
Listing \ref{lst:spark_lazy_loading} zeigt ein Beispiel. Dabei werden die Methoden unterteilt in \textit{Transformationen}, wie beispielsweise \textit{map()} oder \textit{filter()} und \textit{Aktionen}, wie zum Beispiel \textit{first()} oder \textit{reduce()}. Alle Transformationen sind \textit{lazy}. Wenn nun eine Transformation auf einem RDD aufgerufen wird, so wird diese Methode noch nicht auf den Daten ausgeführt. Sie wird erst ausgeführt, wenn auf dem RDD auch eine Aktion gestartet wird. Dies kann die Effizienz der Datenausführung steigern, da nur die Transformationen ausgeführt werden, die für das Ergebnis notwendig sind.

\begin{lstlisting}[label={lst:spark_lazy_loading},caption= Lazy Loading bei Apache Spark ,captionpos=b,frame=single,style=customjava]
JavaRDD<Data> data = getData();
// Definiere Transformationen
data.map(x -> convert(x))
	.filter(Objects::nonNull)
	.map(y -> convertToZ(y))
	// Erst bei dem Aufruf der "Aktion" first(),
	// werden auch die Daten geladen und verarbeitet.
	.first()
\end{lstlisting}

\subsection*{Speicherung der Zwischenergebnisse}
Wie bereits erläutert, unterscheidet Apache Spark bei den Methoden zwischen Transformationen und Aktionen, die auf ein RDD angewendet werden. Meistens werden ein oder mehrere Transformationen auf ein RDD angewendet und zum Schluss folgt eine Aktion, welche das Ergebnis der Datenverarbeitung zurückliefert. In vielen Fällen werden mehrere Ergebnisse berechnet, welche teilweise auf ähnlichen Transformationen aufbauen. Allerdings kann ein RDD, welches durch eine abschließende Aktion einmal ausgeführt wurde, nicht einfach wiederverwendet werden. Spark bietet aber die Möglichkeit, Zwischenergebnisse einzelner RDDs explizit zwischenzuspeichern, um sie für die Berechnung anderer Ergebnisse wiederzuverwenden. Ein Beispiel aus dem forensischen Umfeld ist die Berechnung von Datei-Hashsummen. Oftmals sollen auch Datei-Duplikate ermittelt werden. Das sind zwei unterschiedliche Ergebnismengen, wobei die Ermittlung der Duplikate auf der Berechnung der Hashsummen aufbaut. Prinzipiell müssen zwei unabhängige RDDs erstellt werden. Beide RDDs berechnen im ersten Schritt die Hashsummen der Rohdaten. Diese Arbeit wird bei zwei unabhängigen RDDs auch zweifach ausgeführt und benötigt daher doppelt so viele Ressourcen. Daher wäre es in diesem Beispiel sinnvoll, die Berechnung der Hashsummen zwischenzuspeichern und die Ermittlung von Dateiduplikaten darauf aufzubauen. Bei Apache Spark ist es möglich, die Daten im Arbeitsspeicher zwischenzuspeichern oder falls der Speicher nicht ausreicht, auf einen persistenten Datenspeicher auszuweichen. Allerdings müssen die Daten serialisierbar sein. Dies ist gerade bei komplexeren Datentypen nicht immer gegeben. Das Zwischenspeichern von Daten (Caching) spielt bei Apache Spark eine wichtige Rolle, um die Geschwindigkeit der Datenverarbeitung zu optimieren.\cite{spark_rdd}

\subsection*{Verteilte Datenverarbeitung}
%Balancing and Repartitionieren. Aufteilung der Last zu gleichen Teilen!
Bei der lokalen Entwicklung von Spark-Anwendung ist es einfach, die Resultate eines RDDs über eine Konsole auszugeben. Dadurch kann schnell geprüft werden, ob die Ergebnisse aus fachlicher Sicht korrekt sind. Was hingegen auf kleinen lokalen Testknoten nicht geprüft werden kann, ist die korrekte Skalierung bei großen Datenmengen. Hierbei muss genau überlegt werden, wie die Ergebnisse ausgegeben werden sollen (siehe Listing \ref{lst:spark_rdd_collect}). 

\begin{lstlisting}[label={lst:spark_rdd_collect},caption= Anzeige der Ergebnisse eines Spark RDDs ,captionpos=b,frame=single,style=customjava]
HbaseReader hbr = new HbaseReader(jsc, hbaseConfigFile);
JavaRDD<Metadata> forensicMetadata = hbr.getForensicMetadata();

// collect()-Methode
forensicMetadata.collect().stream()
	.forEach(m -> LOGGER.info("Entry = .", m));

// take(int amount)-Methode	
forensicMetadata.take(10).stream()
	.forEach(m -> LOGGER.info("Entry = .", m));	
\end{lstlisting}

\noindent
In der ersten Variante wird auf dem RDD die Methode collect() aufgerufen und die daraus erhaltene Liste von Objekten wird über ein Logger-Objekt in das Log-File dieser Ausführung geschrieben.\\
Auf den ersten Blick ist aber nicht ersichtlich, was diese Methode wirklich bewirkt. Wie bereits in Kapitel \ref{sec:theory_spark} beschrieben, wird bei der Ausführung einer Spark-Anwendung ein sogenannter Spark-Driver gestartet. Dieser wiederum fordert eine gewisse Anzahl von Executoren an, welche die eigentliche Datenverarbeitung übernehmen (Master-Slave-Prinzip). Die Executoren arbeiten auf einzelnen Knoten innerhalb des Clusters. 
Ein Spark RDD wird bei der Verarbeitung auf diese Executoren aufgeteilt. Doch je nachdem, welche \textit{Aktion} die Ergebnismenge des RDDs zurückliefert, wird das Ergebnis weiterhin verteilt gespeichert oder aber es wird an den Spark-Driver zurückgesendet. In dem Moment, in welchem beispielsweise die collect()-Methode auf einem RDD ausgeführt wird, werden die Daten des RDDs \textit{eingesammelt}. 
Dies bedeutet, dass die Ergebnisse des RDDs, welche verteilt in allen Executoren gespeichert sind, nun an den Spark-Driver geschickt werden.\\
Dieser Mechanismus ist für sich genommen nicht problematisch und funktioniert beim Testen mit kleinen Datenmengen. Bei großen RDDs werden auch alle Ergebnisse an den Driver geschickt. Da der Driver aber nur begrenzte Ressourcen zugeteilt bekommt, wird in den meisten Fällen der Arbeitsspeicher ausgehen und die Anwendung wird abgebrochen.\\
Daher ist es auch schon beim Testen mit kleinen RDDs sinnvoll, die Daten entweder verteilt auszugeben oder die Ergebnismenge, welche an den Driver gesendet wird, zu begrenzen. Im zweiten Teil von Listing \ref{lst:spark_rdd_collect}  wird hierzu die take()-Methode verwendet. Diese verhält sich wie  die collect()-Methode mit dem Zusatz, dass sie nur eine bestimmte Anzahl von Einträgen einsammelt. Dadurch wird bei größeren RDDs der Speicher nicht ausgehen.\\
Wenn möglich, sollten die Ergebnisse immer verteilt gespeichert werden. Die Methode \textit{saveAsTextFile()} speichert die Ergebnisse eines Spark RDDs in das HDFS. Hierbei legt jeder Executor eine eigene Textdatei im HDFS an und speichert dort seinen Anteil der Ergebnismenge. Dadurch ist eine optimal verteilte Datenverarbeitung, auch bei großen Datenmengen, ohne Ressourcenengpass möglich. 


\section{Leistungsanalyse}
\label{sec:performance_analysis}
Die hier entwickelte forensische Analyseplattform soll den Forensiker dabei unterstützen, die Daten schnell verarbeiten und analysieren zu können. Gerade die Dauer der Datenverarbeitung spielt eine wichtige Rolle. In diesem Kapitel soll daher getestet werden, wie lange die Datenaufbereitung dauert. Der gesamte Verarbeitungsprozess der Analyseplattform besteht einerseits aus dem Datenimport und andererseits aus der eigentlichen Datenverarbeitung. Wobei die reine Datenverarbeitung aus der Berechnung der Hashsummen und der Ermittlung der Medientypen besteht.\\
Da im Hadoop-Cluster mehrere Knoten im Verbund arbeiten und unterschiedliche Technologien eingesetzt werden, ist es sehr schwer, zuverlässige Testszenarien durchzuführen. Daher sollten die Ergebnisse dieser Testreihe vielmehr Anhaltspunkte liefern, in welchem groben Zeitraum wie viele Daten verarbeitet werden können.\\
Das hier genutzte Testcluster besteht aus 3 Knoten und bildet daher das kleinste mögliche Hadoop-Cluster mit der standardmäßigen dreifachen Datenreplikation. Die drei Knoten sind identisch und enthalten jeweils eine Intel Core i5-6500 Quad-Core CPU ohne Hyperthreading, 32 GB Arbeitsspeicher, eine 512 GB SSD und eine 2 TB große Magnetfestplatte. Als Betriebssystem wird CentOS 7 (Version 7.5.1804) genutzt. Darauf ist die Hortonworks Data Platform in der Version 2.6.5 installiert. Während das Betriebssystem jeweils auf der SSD installiert ist, werden die Daten aus dem HDFS auf den langsameren und größeren Magnetfestplatten gespeichert. Dadurch hat das HDFS eine maximale Gesamtkapazität von knapp 6 TB. Die Knoten kommunizieren in einem eigenen Netzwerk über Gigabit-Ethernet.\\  

\noindent
Tabelle \ref{tab:performance_results} zeigt die Zeitdauer des Importvorgangs und der Datenverarbeitung. Das erste Datenträgerabbild ist knapp 10 GB groß und enthält eine Ubuntu Installation. Das zweite Datenträgerabbild ist ungefähr 155 GB groß und enthält eine Windows 10 Installation. Letzteres Abbild wird in unterschiedlichen Varianten importiert. Jeder Test wird dreimal durchgeführt. Aus den drei Durchläufen wird jeweils der Mittelwert gebildet.\\

\begin{table}[ht]
\centering
\begin{tabular}{l|l|l}
Datenträgerabbild & Dauer des Datenimports & Dauer der Datenverarbeitung	\\ \hline
Ubuntu Image  10 GB		& 8 min 26 sec	& 4 min 45 sec  \\ \hline
Windows Image - 1. Variante 		& 4 h 32 min		& 31 min 6 sec	\\
Windows Image - 2. Variante 		& 3 h 20 min 	& 29 min 50 sec \\
Windows Image - 3. Variante 		& 1 h 22 min		& 29 min 37 sec \\
\end{tabular}
\caption{Zeitdauer der Datenaufbereitung}
\label{tab:performance_results}
\end{table}

\noindent
Der erste Testdurchlauf mit dem 10 GB großen Ubuntu-Datenträgerabbild ist in 13-14 Minuten abgeschlossen. Das Datenträgerabbild wird von einer SSD auf einem Cluster-Knoten ausgelesen und importiert. Die Zeitdauer von knapp 15 Minuten ist akzeptabel und kann noch optimiert werden.\\

\noindent
Beim Datenimport des Windows-Datenträgerabbildes zeigen sich deutliche Geschwindigkeitsunterschiede. In der ersten Variante befindet sich das Datenträgerabbild auf einer herkömmlichen Festplatte mit Magnetscheiben, welche sich mit 5400 Umdrehungen pro Minute drehen können. Diese Festplatte wird via USB 3.0 mit einem Knoten des Clusters verbunden und die Daten werden importiert. Diese Variante entspricht einem realistischem Szenario, wenn beispielsweise die Festplatte eines Laptops sichergestellt wird und analysiert werden soll.\\
Allein der Datenimport benötigt mehr als 4,5 Stunden. Interessanterweise ist das Hadoop-Cluster nicht einmal annähernd voll ausgelastet. Über die Monitoring-Oberfläche des Clusters zeigt sich, dass die CPU- und Speicher-Auslastung nicht sonderlich hoch ist. Allerdings ist der Datendurchsatz von HBase sehr niedrig. Bei näherer Betrachtung zeigt sich, dass in HBase mehrere sogenannte \textit{Region Splits} durchgeführt werden. Wie in Kapitel \ref{sec:theory_hbase} beschrieben, wird eine HBase-Tabelle in Regions unterteilt. Wenn nun eine Region größer als 10 GB ist, dann teilt HBase diese Region in zwei neue kleinere Regions auf.\footnote{Die Standardkonfiguration von maximal 10 GB Größe kann über den Konfigurationsparameter \textit{hbase.hregion.max.filesize} verändert werden.} Diese \textit{Regions-Splits} benötigen viel Zeit, da die Daten umstrukturiert werden müssen.\\

\noindent
In der zweiten Variante aus Tabelle \ref{tab:performance_results} wird schon beim Anlegen der HBase-Tabelle angegeben, dass insgesamt 30 Regions in HBase erstellt werden sollen. Darüber hinaus wird der Zeilenschlüssel der einzelnen Einträge so modifiziert, dass HBase alle importierten Dateien gleichmäßig auf die 30 Regions verteilt. Wenn nun die Daten des Windows-Datenträgerabbildes importiert werden, dann reichen diese 30 Regions aus, um alle Daten in HBase zu speichern. Es werden keine zeitaufwendige Region-Splits durchgeführt. Dadurch kann die Zeitdauer des Importvorgangs um mehr als eine Stunde verringert werden.\\

\noindent
Die dritte Variante, zum Importieren des Windows-Datenträgerabbildes, basiert auf der zweiten Variante. Dieses Mal ist das Datenträgerabbild auf einer SSD gespeichert und wird direkt von dieser SSD beim Importvorgang ausgelesen. Aufgrund der deutlich höheren Lesegeschwindigkeit von SSDs gegenüber klassischen HDDs mit Magnetscheiben, kann eine höhere Performanz erzielt werden. Daher benötigt der Datenimport in dieser Variante 1 Stunde und 22 Minuten. Hier zeigt sich, dass beim Optimieren der Verarbeitungsgeschwindigkeit die Software und auch die verwendete Hardware gleichermaßen betrachtet werden müssen.\\

\noindent
Die bisher erzielte Verarbeitungsgeschwindigkeit ist akzeptabel. Sie kann aber noch deutlich verbessert werden. Beispielsweise ist die Parallelität beim Datenimport noch nicht optimal implementiert. So kann das Hochladen von großen Dateien in das HDFS und das Speichern der Metadaten in HBase vollständig parallelisiert werden.\\

\noindent
Die Geschwindigkeit beim Datenimport ist auch abhängig von den genutzten Festplatten. Das HDFS im Testcluster ist so konfiguriert, dass die Daten auf die langsameren magnetischen Festplatten mit 7200 Umdrehungen pro Minute geschrieben werden. 
Hier kann die Performanz des Clusters weiter erhöht werden, wenn SSDs die Daten des HDFS speichern. Allerdings ist dieser Ansatz aus Kostengründen nicht immer realisierbar.\\

\clearpage
\noindent
Der Datendurchsatz wird auch erhöht, wenn mehrere Asservate parallel in das Analysesystem importiert werden. 
Damit kann eine optimale Auslastung des Hadoop-Clusters erreicht werden. Zumal bei forensischen Analysen oftmals mehrere Asservate sichergestellt werden und diese dann auch parallel verarbeitet werden sollen.\\

\noindent
Andererseits macht es aus Anwendersicht keinen Sinn, zuerst das Asservat vollständig zu importieren und erst danach mit der Aufbereitung und Analyse zu beginnen. Die Daten können mit Spark auch schon verarbeitet werden, sobald die ersten Dateien im HDFS oder in HBase gespeichert werden. Durch diesen \textit{Streaming-Modus} hat der Nutzer schon während dem Import die Möglichkeit, die ersten eingehenden Daten zu analysieren. Er müsste nicht warten bis der Import vollständig beendet ist. Auch die Datenindexierung kann schon während dem Importvorgang durchgeführt werden. Dieser Ansatz kann die Verarbeitungszeit nochmals deutlich senken und stellt dem Nutzer schon frühzeitig erste Teilergebnisse zur Verfügung.
In einer zukünftigen Weiterentwicklung sollte daher der Datenimport und die Verarbeitung dahingehend überarbeitet werden.