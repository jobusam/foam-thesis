\chapter{Zusammenfassung}
\label{ch:zusammenfassung}
%TODO Die Ergebnisse kurz kritisch hinterfragen und Antworten ob die Plattform für forensische Analysen passen würde. Hier könnte gesagt werden, dass es zwar noch etliche Aspekte fehlen, aber es durchaus mit Hadoop möglich ist!!!

Die Ergebnisse dieser Arbeit zeigen die Machbarkeit einer forensischen Auswertung von großen Datenmengen in einem Computer-Cluster auf Basis von Apache Hadoop. Um die Daten parallel zu verarbeiten, müssen diese in voneinander unabhängig verarbeitbaren Strukturen gespeichert werden. Hierzu bietet es sich an, die logischen Dateien aus den Datenträgerabbildern einzeln im Analysesystem abzuspeichern.\\
Ein kritischer Aspekt ist die Speicherung beliebig kleiner und großer Dateien für eine optimale Verarbeitung. Als Lösung werden die Vorteile unterschiedlicher Datenhaltungen kombiniert. So werden beliebig große Dateien direkt im verteilten Dateisystem von Apache Hadoop gespeichert. Kleine Dateien werden hingegen in der verteilten spaltenorientierten Datenbank HBase abgelegt. Dort können auch die Metadaten und Analyseergebnisse der Datenverarbeitung persistiert werden.\\
Um mehrere Datenträgerabbilder in das System importieren zu können, existiert eine forensische Fallverwaltung. Diese speichert allgemeine technische und fachliche Daten zu den einzelnen importierten Asservaten.\\

\noindent
Im Rahmen der Datenverarbeitung wird die Berechnung von Hashsummen und das Ermitteln von Medientypen der einzelnen Dateien implementiert. Damit ist es möglich, Duplikate zu erkennen und nach Dateitypen zu filtern. Beispielsweise können dadurch alle Bilder von importierten Asservaten aufgelistet werden.\\
Ein interessanter Punkt zur performanten Analyse ist das Indexieren aller Metadaten und Dateiinhalte. Derzeit werden nur die Metadaten für eine performante Suche indexiert. Das Indexieren von Dateiinhalten ist prinzipiell auch möglich. Allerdings  muss dafür die existierende Implementierung zur verteilten Datenindexierung überarbeitet werden.\\

\noindent
Zur Visualisierung der Daten existiert eine rudimentäre Web-Oberfläche, welche die indexierten Metadaten anzeigen kann. Bei der forensischen Analyse spielen aber auch weitere Aspekte, wie beispielsweise die Datensicherheit und Integrität der Beweismittelkette eine Rolle. Es werden theoretische Ansätze aufgezeigt, wie die Analyseplattform diese Anforderungen erfüllen kann.\\

\noindent
Letztlich bestätigt die hier entwickelte Analyseplattform die Machbarkeit einer verteilten Datenverarbeitung im forensischen Umfeld. Der hier entwickelte Ansatz liefert eine Basis und zeigt auf, wie die Analyseplattform zukünftig verbessert werden kann. Denn die aktuelle Implementierung bietet großes Potential, um die Daten zukünftig schneller aufbereiten und verarbeiten zu können.\\

\chapter{Ausblick}
\label{ch:ausblick}

Die Analyseplattform bietet eine Grundlage zur verteilten Datenverarbeitung bei forensischen Auswertungen. Ausgehend von dieser Grundlage, gibt es viele Möglichkeiten die Plattform zu erweitern und zu verbessern.\\
Bisher existiert für die Analyseplattform nur eine rudimentäre Web-Oberfläche zur Anzeige und Suche auf Basis der Metadaten. Hier müsste eine vollständig neue Oberfläche zur Datenvisualisierung erstellt werden. Ähnlich zu Autopsy sollte es möglich sein, Daten in einer Zeitachse zu visualisieren und Bilder, Videos oder Dokumente direkt anschauen zu können. Ein weiterer interessanter Ansatz wäre die Visualisierung der Beziehungen zwischen den einzelnen Dateien in einer Graphenstruktur. Hier könnte die Graphendatenbank Neo4j und ihre Datenvisualisierung als Vorbild dienen. Es sollte auch entsprechende Möglichkeiten geben, die Daten zu verwalten, zu filtern und auch manuell Informationen hinzuzufügen.\\

\noindent
Ein weiterer wichtiger Punkt ist die Optimierung der Verarbeitungsgeschwindigkeit. In der bisherigen Implementierung werden immer zuerst die Daten eines einzelnen Datenträgerabbildes in das System importiert. Dieser Datenimport kann zwar parallelisiert werden, jedoch erfolgt die eigentliche Datenverarbeitung immer erst nachdem der Datenträger vollständig importiert wurde. Diese Problematik könnte verbessert werden, indem die Plattform zukünftig jede Datei direkt nach ihrer Speicherung im Cluster auch verarbeitet (Streaming Data Modus). Damit kann ein Nutzer schon während des Datenimports auf Teilergebnisse der bereits analysierten Daten zugreifen. Darüber hinaus wäre es dann auch sinnvoll, wenn die Datenimport-Anwendung versuchen würde, fallrelevante Daten zuerst zu importieren.
Beispielweise können bei Datenträgerabbildern mit installierten Betriebssystemen die nutzerspezifischen Verzeichnisse zuerst importiert werden, da diese im Normalfall für den Analyst einen höheren Mehrwert bieten als betriebssystemspezifische Verzeichnisse.\\
Ein anderer Aspekt beim Datenimport ist das Mounten der Datenträgerabbilder. Wie in Kapitel \ref{subsec:data_import_access_rights} beschrieben, wird das Projekt \textit{BindFS} genutzt, um dem Datenimport Zugriff auf alle Dateien zu geben. Dadurch ist es derzeit nicht möglich, den Besitzer und die Gruppe aus den Metadaten einer Datei korrekt auszulesen. Ein zukünftige Weiterentwicklung sollte diese Metadaten korrekt auslesen können.\\

\noindent
Auch die bisherige Datenverarbeitung muss noch erweitert werden. Beispielsweise wäre eine automatische Extraktion gefundener IP-Adressen, Web-Adressen, E-Mail-Adressen und Positionsdaten sinnvoll. Darauf aufbauend wäre es möglich, Beziehungen zwischen mehreren Asservaten und deren Nutzern herzustellen. Die Extraktion spezifischer Nutzerdaten, wie beispielsweise der Browserhistorie oder E-Mails, wäre auch nützlich.\\

\noindent
Die Implementierung einer Volltextsuche für alle Dateiinhalte wurde noch nicht zufriedenstellend gelöst. Bisher wird die Suche nur auf den Dateimetadaten angewendet. Wobei die Komponente zur Datenindexierung (Apache Solr) eigentlich die Möglichkeit bietet, ganze Dateien automatisiert zu indexieren. Allerdings muss hierzu das verwendete Projekt \textit{Lily HBase Indexer} ausgetauscht werden, da diese Implementierung aktuell keine Binärdateien verarbeiten kann und es auch nicht mehr mit den neueren Versionen von HBase kompatibel ist. Hier müssen neue Lösungen gefunden werden, wie die Daten verteilt im Hadoop-Umfeld indexiert werden können.\\

\noindent
Auch die querschnittlichen Aspekte bei der forensischen Analyse müssen in zukünftigen Implementierungen berücksichtigt werden. So muss die Analyseplattform entsprechend abgesichert werden. Zur Authentifizierung eines Nutzers kann Kerberos genutzt werden, da in der Theorie alle verwendete Komponenten auch die Integration von Kerberos unterstützen. Es ist auch möglich, die Daten auf den einzelnen Knoten des Clusters zu verschlüsseln. Dazu bietet das verteilte Dateisystem HDFS entsprechende Optionen an. In diesem Kontext müsste auch ein passendes Rollenkonzept entwickelt werden.\\
Ein weiterer Aspekt ist die automatische Generierung der Beweismittelkette. Da das System alle Nutzeraktionen protokolliert, können diese Informationen genutzt werden, um eine Beweismittelkette zu erstellen. Damit soll die Herkunft der Ergebnisse und die Integrität der zugrunde liegenden Daten sichergestellt werden.\\
Zuletzt muss das forensisch korrekte Löschen von Daten in der Praxis detailliert analysiert werden. Denn nachdem ein Fall bearbeitet wurde, müssen die sensiblen Daten gelöscht werden und können nicht mehr weiterhin im Computer-Cluster verbleiben. Es ist  möglich, die Daten im verteilten HDFS zu löschen. Aber es muss auch garantiert werden, dass keine Rückstände auf den Festplatten der einzelnen Knoten im Cluster vorhanden sind.