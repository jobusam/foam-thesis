\chapter{Zusammenfassung}
\label{ch:zusammenfassung}
%TODO Die Ergebnisse kurz kritisch hinterfragen und Antworten ob die Plattform für forensische Analysen passen würde. Hier könnte gesagt werden, dass es zwar noch etliche Aspekte fehlen, aber es durchaus mit Hadoop möglich ist!!!

Die Ergebnisse dieser Arbeit zeigen die Machbarkeit einer forensischen Auswertung von großen Datenmengen in einem Computer-Cluster auf Basis von Apache Hadoop. Um die Daten parallel zu verarbeiten, müssen diese in voneinander unabhängig verarbeitbaren Strukturen gespeichert werden. Hierzu bietet es sich an, die logischen Dateien aus den Datenträgerabbilder einzeln im Analysesystem abzuspeichern.\\
Ein kritischer Aspekt ist die Speicherung beliebig kleiner und großer Dateien für eine optimale Verarbeitung. Als Lösung werden die Vorteile unterschiedlicher Datenhaltungen kombiniert. So werden beliebig große Dateien direkt im verteilten Dateisystem von Apache Hadoop gespeichert. Kleine Dateien werden hingegen in der verteilten spaltenorientierten Datenbank HBase gespeichert. Dort können auch die Metadaten und Analyseergebnisse der Datenverarbeitung abgelegt werden. Derzeit benötigt der Datenimport in das Analysesystem aber noch viel Zeit und die Geschwindigkeit ist noch nicht zufriedenstellend.\\
Um mehrere Datenträgerabbilder in das System importieren zu können, existiert eine forensische Fallverwaltung. Diese speichert allgemeine technische und fachliche Daten zu den einzelnen importierten Asservaten.\\

\noindent
Als Beispiele zur Datenverarbeitung wird die Berechnung von Hashsummen und das Ermitteln von Medientypen der einzelnen Dateien implementiert. Damit ist es bereits möglich Duplikate zu erkennen und nach Dateitypen zu filtern. Beispielsweise können damit alle Bilder der importierten Datenträger aufgelistet werden.\\
Ein interessanter Punkt zur performanten Analyse ist das Indexieren aller Metadaten und Dateiinhalte. Hierbei werden bereits alle Metadaten für eine performante Suche indexiert. Das Indexieren von Dateiinhalten ist prinzipiell auch möglich. Allerdings  muss dafür die bereits existierende Implementierung zur verteilten Datenindexierung im Computer-Cluster nochmals überarbeitet und verbessert werden.\\

\noindent
Zur Visualisierung der Daten existiert eine rudimentäre Web-Oberfläche, welche die indexierten Metadaten anzeigen kann. Bei der forensischen Analyse spielen aber auch noch weitere Aspekte, wie beispielsweise die Datensicherheit und Integrität der Beweismittelkette eine Rolle. Hierbei existieren theoretische Ansätze, wie die hier entwickelte Analyseplattform diese Anforderungen erfüllen könnte.\\

\noindent
Letztlich bestätigt die hier entwickelte Analyseplattform die Möglichkeit zur verteilten Datenverarbeitung im forensischen Umfeld. Der hier entwickelte Ansatz liefert eine Basis und zeigt auf, wie die Analyseplattform zukünftig verbessert werden kann. Denn die aktuelle Implementierung bietet noch großes Potential, um die Daten zukünftig schneller aufbereiten und verarbeiten zu können.\\

\chapter{Ausblick}
\label{ch:ausblick}

Die hier entwickelte Analyseplattform bietet eine Grundlage zur verteilten Datenverarbeitung bei forensischen Auswertungen. Ausgehend von dieser Grundlage gibt es viele Möglichkeiten die Plattform zu erweitern und zu verbessern.\\
Bisher existiert für die Analyseplattform nur eine rudimentäre Web-Oberfläche zur Anzeige und Suche auf Basis der Metadaten. Hier müsste eine vollständig neue Oberfläche zur Datenvisualisierung erstellt werden. Ähnlich zu Autopsy sollte es möglich sein, Daten in einer Zeitachse zu visualisieren und Bilder, Videos oder Dokumente direkt anschauen zu können. Ein weiterer interessanter Ansatz wäre die Visualisierung der Beziehung zwischen den einzelnen Dateien in einer Graphenstruktur. Hier könnte die Graphendatenbank Neo4j und ihre Datenvisualisierung als Vorbild dienen. Es sollte auch entsprechende Möglichkeiten geben, die Daten zu verwalten, zu filtern und auch manuell Informationen hinzuzufügen.\\

\noindent
Ein weiterer wichtiger Punkt ist die Optimierung der Verarbeitungsgeschwindigkeit. In der bisherigen Implementierung werden immer zuerst die Daten eines einzelnen Datenträgerabbildes in das System importiert. Dieser Datenimport kann zwar parallelisiert werden, jedoch erfolgt die eigentliche Datenverarbeitung immer erst nachdem der Datenträger vollständig importiert wurde. Dieser Datenimport dauert derzeit noch viel zu lange. Hier müsste das System nochmals detailliert analysiert werden, um herauszufinden wieso der Datenimport so lange dauert. \\
Diese Problematik könnte zusätzlich verbessert werden, indem die Plattform zukünftig jede logische Datei direkt nach ihrer Speicherung im Cluster auch verarbeitet (Streaming Data Modus). Damit könnte ein Nutzer schon während des Datenimports auf Teilergebnisse der bereits analysierten Daten zugreifen. Darüber hinaus wäre es dann auch sinnvoll, wenn die Datenimport-Anwendung versuchen würde fallrelevante Daten zuerst zu importieren. Beispielweise könnten bei Datenträgerabbilder mit installierten Betriebssystemen die nutzerspezifischen Verzeichnisse zuerst importiert werden, da diese im Normalfall für den Analyst einen höheren Mehrwert bieten, als betriebssystemspezifische Verzeichnisse.\\
Ein anderer Aspekt beim Datenimport ist das Mounten der Datenträgerabbilder. Wie in Kapitel \ref{subsec:data_import_access_rights} beschrieben, wird das Projekt \textit{BindFS} genutzt, um dem Datenimport Zugriff auf alle Dateien zu geben. Dadurch ist es derzeit nicht möglich den Besitzer und die Gruppe aus den Metadaten einer Datei korrekt auszulesen. Hier könnte in einer Weiterentwicklung geprüft werden, wie diese Metadaten dennoch ausgewertet können.\\

\noindent
Auch die bisherige Datenverarbeitung könnte erweitert werden. So könnten beispielsweise alle gefundenen IP-Adressen, Web-Adressen, E-Mail-Adressen oder Positionsdaten automatisch extrahiert werden. Diese Daten könnten dann wiederum genutzt werden, um Beziehungen zwischen den Asservaten feststellen zu können.
Die Extraktion spezifischer Nutzerdaten, wie beispielsweise der Browserhistorie oder E-Mails, wäre auch sinnvoll.\\

\noindent
Die Implementierung einer Volltextsuche für alle Dateiinhalte wurde auch noch nicht zufriedenstellend gelöst. Bisher wird die Suche nur auf den Dateimetadaten angewendet. Wobei die Komponente zur Datenindexierung (Apache Solr) eigentlich die Möglichkeit bietet, ganze Dateien automatisiert zu indexieren. Allerdings muss hierzu das verwendete Projekt \textit{Lily Hbase Indexer} ausgetauscht werden, da diese Implementierung aktuell keine Binärdateien verarbeiten kann und es auch nicht mehr mit den neueren Versionen von HBase kompatibel ist. Hier müssten neue Lösungen gefunden werden, wie die Daten verteilt im Hadoop-Umfeld indexiert werden können.\\

\noindent
Aber auch die querschnittlichen Aspekte bei der forensischen Analyse müssen in zukünftigen Implementierungen berücksichtigt werden. So muss die Analyseplattform entsprechend abgesichert werden. Hierzu könnte zur Nutzerauthentifikation Kerberos genutzt werden, da in der Theorie alle verwendete Komponenten auch die Integration von Kerberos unterstützen. Es wäre auch möglich die Daten auf den einzelnen Computer-Knoten zu verschlüsseln. Hierbei bietet das verteilte Dateisystem HDFS entsprechende Möglichkeiten. In diesem Kontext müsste auch ein passenden Rollenkonzept entwickelt werden.\\
Ein weiterer Aspekt ist die automatische Generierung der Beweismittelkette. Da das System automatisch die Nutzeraktionen protokolliert, könnten diese Informationen genutzt werden, um eine Beweismittelkette zu erstellen. Damit soll die Herkunft der Ergebnisse und die Integrität der zugrunde liegenden Daten sichergestellt werden.\\
Zuletzt müsste auch das forensisch korrekte Löschen von Daten nochmals praktisch analysiert werden. Denn nachdem ein Fall bearbeitet wurde, müssen die sensiblen Daten gelöscht werden und können nicht mehr weiterhin im Computer-Cluster verbleiben. Hierbei ist es zwar möglich die Daten im verteilten HDFS aus logischer Sicht zu löschen. Aber es muss auch garantiert werden, dass keine Rückstände auf den Festplatten der einzelnen  Computer-Knoten vorhanden sind.