\chapter{Zusammenfassung}
\label{ch:zusammenfassung}
\textbf{TODO: 2 Seiten Zusammenfassung}!\\
Die Ergebnisse dieser Arbeit zeigen die Machbarkeit einer forensischen Auswertung von großen Datenmengen in einem Computer-Cluster auf Basis von Apache Hadoop. Um die Daten parallel zu verarbeiten, müssen diese in voneinander unabhängig verarbeitbaren Strukturen gespeichert werden. Hierzu bietet es sich an, die Daten als getrennte logische Dateien abzuspeichern, so wie sie aus logischer Sicht in den Datenträgerabbildern persistiert sind.\\
Ein kritischer Aspekt ist die Speicherung beliebig kleiner und großer Dateien für eine optimale Verarbeitung. Als Lösung werden die Vorteile unterschiedlicher Datenhaltungen kombiniert. So werden beliebig große Dateien direkt im verteilten Dateisystem von Apache Hadoop gespeichert. Kleine Dateien, kleiner 10 Megabyte, werden hingegen in der verteilten spaltenorientierten Datenbank HBASE gespeichert. Dort können auch die Metadaten und Analyseergebnisse der Datenverarbeitung abgelegt werden.\\

\noindent
Als Beispiele zur Datenverarbeitung wird die Berechnung von Hashsummen und das Ermitteln von Medientypen der einzelnen Dateien implementiert. Damit ist es bereits möglich Duplikate zu erkennen und nach Dateitypen zu filtern. Beispielsweise können alle Bilder der importierten Datenträger aufgelistet werden.\\
Ein interessanter Punkt zur performanten Analyse ist das Indexieren aller Metadaten und Dateiinhalte. Hierbei werden bereits alle Metadaten für eine performante Suche indexiert. Das Indexieren von Dateiinhalten ist prinzipiell auch mit Apache Solr möglich. Allerdings  muss an dieser Stelle die bereits existierende Implementierung zur verteilten Datenindexierung im Computer-Cluster überarbeitet und verbessert werden.\\

\noindent
Letztlich wurde auch eine simple Web-Oberfläche erstellt, um die Metadaten auswerten zu können.\\
Bei der forensischen Analyse spielen aber auch noch weitere Aspekte, wie beispielsweise die Datensicherheit und Integrität der Beweismittelkette eine Rolle. Hierbei existieren theoretische Ansätze, wie die hier entwickelte Analyseplattform diese Anforderungen erfüllen könnte.\\

\noindent
Letztlich bestätigt die hier entwickelte Analyseplattform die Möglichkeit zur verteilten Datenverarbeitung im forensischen Umfeld. Der hier entwickelte Ansatz liefert eine Basis und zeigt aber auch, wie die Analyseplattform verbessert werden kann um zukünftig schneller und effizienter Daten verarbeiten zu können.\\


\chapter{Ausblick}
\label{ch:ausblick}

Die hier entwickelte Analyseplattform liefert eine Basis zur verteilten Datenverarbeitung bei forensischen Auswertungen. Ausgehend von dieser Basis gibt es viele Möglichkeiten die Plattform zu erweitern und zu verbessern.

Bisher existiert für die Analyseplattform nur eine rudimentäre Web-Oberfläche zur Anzeige und Suche auf Basis der Metadaten. Hier müsste eine vollständig neue Oberfläche zur Datenvisualisierung erstellt werden. Ähnlich zu Autopsy sollte es möglich sein, Daten in einer Zeitachse zu visualisieren und Bilder, Videos oder Dokumente direkt anschauen zu können. Ein weiterer interessanter Ansatz wäre die Visualisierung der Beziehung zwischen den einzelnen Dateien in einer Graphenstruktur. Hier könnte die Graphendatenbank Neo4j und ihre Datenvisualisierung als Vorbild dienen. Es sollte auch entsprechende Möglichkeiten geben, die Daten zu verwalten, zu filtern und auch manuell Informationen hinzuzufügen.\\

\noindent
Ein weiterer wichtiger Punkt ist die Optimierung der Verarbeitungsgeschwindigkeit. In der bisherigen Implementierung werden immer zuerst die Daten eines einzelnen Datenträgerabbildes in das System importiert. Dieser Datenimport kann zwar parallelisiert werden, jedoch erfolgt die eigentliche Datenverarbeitung immer erst nachdem der Datenträger vollständig importiert wurde. Jedoch benötigt dieser Datenimport viel Zeit, gerade wenn auch die Daten über ein Netzwerk mit begrenzter Bandbreite transportiert werden müssen. Diese Problematik könnte verbessert werden, indem die Plattform zukünftig jede logische Datei direkt nach ihrer Speicherung im Cluster auch verarbeitet (Streaming Data Modus von Apache Spark). Hierdurch könnte der Analyse schon während des Importvorgangs der Daten auf Teilergebnisse zugreifen. Darüber hinaus wäre es dann auch sinnvoll, wenn die Datenimport-Anwendung versucht fall-relevante Daten zuerst zu importieren. Beispielweise könnten bei Datenträgerabbilder mit installierten Betriebssystemen die nutzerspezifischen Verzeichnisse zuerst importiert werden, da diese im Normalfall für den Analyst einen höheren Mehrwert bieten, als betriebssystemspezifische Verzeichnisse.\\
Ein anderer Aspekt beim Datenimport ist das Mounten der Datenträgerabbilder. Wie in Kapitel \ref{subsec:data_import_access_rights} wird das Projekt \textit{BindFS} genutzt, um dem Datenimport Zugriff auf alle Dateien zu geben. Dadurch ist es derzeit nicht möglich den Besitzer und die Gruppe aus den Metadaten einer Datei korrekt auszulesen. Hier könnte in einer Weiterentwicklung geprüft werden, wie diese Metadaten dennoch ausgewertet können.\\

\noindent
Auch die bisherige Datenverarbeitung könnte deutlich erweitert werden. So könnten beispielsweise alle gefunden IP-Adressen, Web-Adressen, E-Mail-Adressen oder Positionsdaten automatisch extrahiert werden. Diese Daten könnten dann wiederum genutzt werden um Beziehungen zwischen einzelnen Asservaten und auch Nutzern feststellen zu können. \\
Auch die Extraktion anwendungsspezifischer Nutzerdaten, wie beispielsweise der Browserhistorie oder E-Mails wäre sinnvoll.\\
Ein weiterer Punkt ist auch die Implementierung einer Volltextsuche für alle Dateiinhalte. Bisher wird die Suche nur auf den Dateimetadaten angewendet. Wobei Apache Solr schon direkt die Möglichkeit bietet ganze Dateien automatisiert zu indexieren. Allerdings muss hierzu das verwendete Projekt \textit{Lily Hbase Indexer} neu überarbeitet werden. Da diese Implementierung nicht Binärdateien verarbeiten kann und weil dieses Projekt auch nicht mehr mit den neueren Version von HBASE (ab Version 2.0.0) funktioniert. Hier müssten neue Wege ermittelt werden, wie die Daten verteilt im Hadoop-Umfeld indexiert werden können.\\

\noindent
Aber auch die querschnittlichen Aspekte bei der forensischen Analyse müssen in zukünftigen Implementierungen mit integriert werden. So muss die Analyseplattform entsprechend abgesichert werden. Hierzu könnte zur Nutzerauthentifikation Kerberos genutzt werden, da in der Theorie alle genutzten Komponenten auch die Integration von Kerberos unterstützten. Es wäre auch möglich die Daten auf den einzelnen Computer-Knoten zu verschlüsseln. Hierbei bietet das verteilte Dateisystem HDFS entsprechende Möglichkeiten.\\ In diesem Kontext müsste auch ein passenden Rollenkonzept entwickelt werden.\\ Ein weiterer Aspekt ist auch die automatische Generierung der Beweismittelkette. Da das System automatisch die Nutzeraktionen protokolliert, könnten diese Informationen genutzt werden, um eine Beweismittelkette zu erstellen. Damit soll die Herkunft der Ergebnisse und die Integrität der zugrunde liegenden Daten sichergestellt werden.\\
Zuletzt müsste auch das forensisch korrekte Löschen Daten nochmals praktisch analysiert werden. Denn nachdem ein Fall bearbeitet wurde, müssen die sensiblen Daten gelöscht werden und können nicht mehr weiterer im Computer-Cluster gespeichert werden. Hierbei ist es zwar möglich die Daten im verteilten HDFS aus logischer Sicht zu löschen. Aber es muss auch garantiert werden, dass keine Rückstände auf den Festplatten der einzelnen  Computer-Knoten vorhanden sind. Diese Garantie ist deutlich schwieriger einzuhalten.