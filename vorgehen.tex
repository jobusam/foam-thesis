\chapter{Vorgehen}
\label{ch:development_approach}

\section{Projektplanung}
\label{sec:project_plan}
Zur Realisierung einer forensischen Analyseplattform wurde ein Projektplan erstellt, welcher die einzelnen Aufgaben im Rahmen der Masterthesis enthält. Abbildung \ref{fig:workpackages} zeigt die Aufteilung in diese Arbeitspakete.\\
Das Ziel der Einarbeitungsphase ist es, ein grundlegendes Verständnis über die Datenverarbeitung im Hadoop-Framework zu erhalten. Zusätzlich soll eine Entwicklungsumgebung inklusive öffentlicher Versionsverwaltung eingerichtet werden. Danach erfolgt der Aufbau eines eigenen Hadoop-Clusters und die Beschaffung von Testdaten.\footnote{Hierbei wird ein bestehendes Hadoop-Cluster genutzt und um zusätzliche Softwarepakete ergänzt.} Für die Einarbeitung und den Aufbau sind vier Wochen eingeplant (siehe Abbildung \ref{fig:ganttA}).\footnote{Die referenzierten Gantt-Diagramme wurden mit der JavaScript-Bibliothek \textit{dhtmlxGantt} erstellt. Der Quellcode ist unter der \textit{GNU GPLv2}-Lizenz lizenziert. Weitere Informationen können in Kapitel \ref{sec:licencing_issues} im Anhang nachgelesen werden.}\\

\noindent
Der zweite Teil behandelt die Datenaufbereitung und Speicherung im Hadoop-Cluster. Es soll geprüft werden, welche Struktur der Daten für eine optimale Speicherung und Verarbeitung im Hadoop-Framework erforderlich ist. Für diesen Teil sind vier Wochen Bearbeitungszeit geplant.\\
Am Ende des Arbeitspakets soll ein erster Zwischenbericht erstellt werden, welcher die bisherigen Ergebnisse enthält (siehe Abbildung \ref{fig:ganttA}).\\

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{./resource/Arbeitspakete.pdf}
  \caption{Arbeitspakete der Masterthesis}
  \label{fig:workpackages}
\end{figure}

\noindent
Nach der Speicherung der Rohdaten erfolgt im dritten Arbeitspaket die Datenanalyse mit Apache Spark. Hier sollen die Daten nach anwendungsbezogenen Problemstellungen analysiert werden. Ein weiterer Aspekt der Datenanalyse beschäftigt sich mit den Möglichkeiten, wie die Ergebnisse persistiert werden können.\footnote{Hierfür soll Apache HBase zur Speicherung von strukturierten und unstrukturierten Daten untersucht werden.} Im Anschluss soll die Performanz der Algorithmen geprüft werden. Hier bietet sich der Vergleich zu herkömmlichen Analyseprogrammen an. Denn schließlich hat diese Thesis auch das Ziel, bei großen Datenmengen schneller Ergebnisse zu liefern als die herkömmlichen Analysewerkzeuge auf einem einzelnen Analyserechner. Für dieses Arbeitspaket sind sieben Wochen eingeplant (siehe Abbildung \ref{fig:ganttB}).\\
 Darauf folgt ein zweiter Zwischenbericht.\\

\noindent
Im letzten Drittel der Masterthesis sollen querschnittliche Aspekte in die bestehende Analyseplattform integriert werden. Hierbei geht es um das Absichern der Plattform, die Dokumentation der Beweismittelkette und um das sichere Löschen von Asservaten. Für dieses Arbeitspaket sind vier Wochen eingeplant (siehe Abbildung \ref{fig:ganttC}).\\

\noindent
Das letzte Arbeitspaket enthält ein prototypische Visualisierung der Analyseergebnisse. Hierbei soll geprüft werden, welche Möglichkeiten zur Darstellung der Ergebnisse existieren. Für diese Arbeit sind drei Wochen eingeplant (siehe Abbildung \ref{fig:ganttC}).\\

\subsection*{Projektverlauf}
Während dem Projektverlauf wurde die Planung teilweise angepasst. Es wurden einige Aspekte aus der Planung entfernt (orange hinterlegt in Abbildung \ref{fig:workpackages}). So werden die Visualisierung der Ergebnisse, die Erstellung der Beweismittelkette, das Absichern der Analyseplattform und die forensisch korrekte Datenlöschung nicht implementiert, sondern nur theoretisch erläutert. Der Hauptgrund dafür ist eine verlängerte Analyse- und Implementierungsphase zum Importieren und Speichern von Asservaten im Hadoop-Framework. Es wurden mehrere Ansätze entwickelt und getestet. Dadurch verlängerte sich die ursprünglich angedachte Bearbeitungszeit.\\

\noindent
Andererseits sind auch neue Arbeitspakete hinzugekommen (blau hinterlegt in Abbildung \ref{fig:workpackages}). So wurde bei der Datenanalyse mit Apache Spark sichtbar, dass die Informationen und Analyseergebnisse performant durchsuchbar sein müssen. Daher wurde untersucht, wie eine Volltextsuche aller gespeicherten Daten im Hadoop-Cluster realisiert werden könnte.\\
Ein anderer Aspekt ist die Implementierung einer Fallverwaltung. Denn damit können nun mehrere Asservate in das System importiert werden, um Zusammenhänge identifizieren zu können. 

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./resource/ganttA.png}
  \caption{Projektplan Teil A - Einarbeitung und Rohdatenspeicherung (siehe Kapitel \ref{sec:licencing_issues})}
  \label{fig:ganttA}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./resource/ganttB.png}
  \caption{Projektplan Teil B - Datenanalyse (siehe Kapitel \ref{sec:licencing_issues})}
  \label{fig:ganttB}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./resource/ganttC.png}
  \caption{Projektplan Teil C - Querschnittliche Aspekte und Visualisierung (siehe Kapitel \ref{sec:licencing_issues})}
  \label{fig:ganttC}
\end{figure}

\clearpage
\section{Entwicklungsumgebung}
\label{development_environment}
Der Aufbau einer Test- und Entwicklungsumgebung ist ein wichtiger Bestandteil dieser Thesis. Einerseits sollen Anwendungsprogramme zur Datenverarbeitung schnell und lokal ausführbar sein. Andererseits soll die Testumgebung auf einem realen Apache Hadoop Cluster basieren, um mögliche Infrastrukturprobleme identifizieren zu können und die Performanz zu testen. \\

\noindent
Abbildung \ref{fig:development_environment} skizziert die Komponenten der Entwicklungsumgebung. Zentraler Bestandteil ist ein Entwicklungsrechner mit der Linux-Distribution \textit{Fedora} in der Version 28 64-bit.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{./resource/development_environment.pdf}
  \caption{Komponenten der Entwicklungsumgebung}
  \label{fig:development_environment}
\end{figure} 

\noindent
Zur Entwicklung der forensischen Analyseprogramme wird \textit{Eclipse Oxygen} genutzt. Die Anwendungen selbst werden in Java geschrieben.\footnote{Wobei auch Python oder Scala als Programmiersprache genutzt werden kann.} Zum Bauen der ausführbaren \gls{jar} wird \textit{Maven} verwendet. Mit Maven können weitere Java-Bibliotheken 
in eigenen Programmen auf einfache Weise wiederverwendet werden.\footnote{Diese können über ein zentrales Repository, dem sogenannten \textit{Maven Central Repository} aus dem Internet geladen werden (siehe Link \url{https://search.maven.org/}. Letzter Zugriff 26.8.2018).}\\

\noindent
Zusätzlich befindet sich die Entwicklungsumgebung \textit{IntelliJ IDEA} in der kostenlosen Cummunity Variante  auf dem Entwicklungsrechner. Mit der IntelliJ IDEA wird die Datenimport-Anwendung in \textit{Kotlin} entwickelt. Kotlin ist eine statisch typisierte Programmiersprache zur Anwendungsentwicklung auf verschiedenen Plattformen.\footnote{Siehe Link \url{https://kotlinlang.org/}. Letzter Zugriff: 24.8.2018.} Sie ist interoperabel mit Java. Die Anwendungen können in der \textit{Java Virtual Machine} (JVM) ausgeführt werden. Gegenüber Java bietet sie diverse Sprachkonstrukte zur Optimierung des Programmcodes an. Darüber hinaus können alle Bibliotheken aus dem Java-Umfeld auch in Kotlin genutzt werden. Zum Bauen der Kotlin-Anwendungen wird \textit{Gradle} genutzt, welches analog zu Maven Abhängigkeiten zu Drittbibliotheken und deren Versionen verwaltet.\footnote{Siehe auch Kapitel \ref{subsec:data_import_implementation} für weitere Informationen zur Datenimportanwendung.}\\


\noindent
Um die gebauten Java- und Kotlin-Programme schnell zu testen, können alle notwendigen Komponenten auch lokal auf dem Entwicklungsrechner gestartet werden. Hierzu gehört ein Hadoop-Knoten im sogenannten \textit{Pseudo-Distributed} Modus, eine lokale Spark-Instanz, eine HBase-Instanz und eine Solr-Instanz zur Datenindexierung.\footnote{Siehe Kapitel \ref{ch:theory_hadoop} für eine detaillierte Erklärung der Komponenten.}\\
Mithilfe dieser Komponenten können auch spezifische Konfigurationen getestet werden.\footnote{Hierfür muss der Entwicklungsrechner entsprechende Ressourcen bereitstellen. Es sollte mindestens eine Quad-Core-CPU, 16 GB Arbeitsspeicher und eine SSD zur Verfügung stehen, um performant arbeiten zu können.} \\
Letztendlich kommen die lokalen Instanzen schnell an ihre Grenzen, gerade wenn größere Datenmengen analysiert werden sollen. Daher werden spezifische Konfigurationen und fertiggestellte Analyseprogramme auch auf einem realen
Apache Hadoop-Cluster durchgeführt. Dort kann das Zusammenspiel zwischen den Komponenten nachvollzogen werden. Auch entsprechende Last-Tests sind nur auf dem Hadoop Test-Cluster möglich. Um mit dem Test-Cluster arbeiten zu können, wird ein SSH-Client benötigt. Zusätzlich gibt es eine Web-Oberfläche, basierend auf Apache Ambari, zur Konfiguration und Anzeige des aktuellen Systemzustandes.\\

\noindent
Alle selbst erstellten Anwendungsprogramme, Konfigurationsdateien und die Dokumentation dieser Thesis sollen als Open-Source Projekte in einem öffentlichen Repository zugänglich sein. Gerade in der Forensik ist es sehr wichtig, dem Nutzer die Möglichkeit zu geben, den Quellcode der Analyseprogramme einsehen zu können und notfalls auf spezielle Bedürfnisse anzupassen. Darüber hinaus kann die Datenverarbeitung transparent nachvollzogen werden.
Daher werden die einzelne Projekte mithilfe eines Git-Clients auf GitHub versioniert.\\
Nachfolgende Auflistung zeigt die Aufteilung der Projekte:
\begin{itemize}
\item Das Projekt \textit{foam-thesis}\footnote{Die Abkürzung \textit{foam} oder auch \textit{\acrshort{foam}} steht für \textit{\textbf{fo}rensische \textbf{A}nalyseplattfor\textbf{m}}.} enthält die schriftliche Ausarbeitung der Thesis und den Quellcode als Latex-Projekt. Als Entwicklungsumgebung wird \textit{Texmaker} genutzt.\\
Über den Link \url{https://github.com/jobusam/foam-thesis} ist der aktuelle Stand der Arbeit jederzeit einsehbar.\footnote{Das erstellte PDF-Dokument zum jeweiligen Stand wird im gleichen Projekt versioniert und ist unter dem Link \url{https://github.com/jobusam/foam-thesis/blob/master/main.pdf} verfügbar (Letzter Zugriff: 6.10.2018).}

\item Das Projekt \textit{foam-data-import} enthält den Quellcode zum Importieren von Asservaten in das Hadoop-Cluster. Unter \url{https://github.com/jobusam/foam-data-import} befindet sich die Kotlin-Anwendung, welche wiederum mit Gradle gebaut werden kann.

\item Das Projekt \textit{foam-processing-spark} enthält den Quellcode zur Auswertung mit Apache Spark\texttrademark. Unter \url{https://github.com/jobusam/foam-processing-spark} befindet sich ein Maven-Projekt, welches wiederum die Java-Anwendung baut. Es werden auch entsprechende Skripte zum Starten von Spark-Anwendungen für lokale Installationen und Hadoop-Cluster bereitgestellt. 

%TODO Installationsskripte zu HBase und Solr!
\item Das Projekt \textit{foam-storage-hadoop} enthält alle Konfigurationsdateien zum Aufsetzen eines Hadoop-Clusters auf einem einzelnem Knoten im \textit{Pseudo-Distributed Mode}.\footnote{Siehe Link: \url{https://github.com/jobusam/foam-storage-hadoop/tree/master/hadoop.standalone.configuration}. Letzter Zugriff: 6.10.2018.} Zusätzlich existieren Shell-Skripte zum Starten des Hadoop-Clusters auf einem einzelnen Knoten.\footnote{Siehe Link: \url{https://github.com/jobusam/foam-storage-hadoop/tree/master/hadoop.standalone.setup}. Letzter Zugriff: 6.10.2018.} Mithilfe der Skripte aus dem \textit{foam-processing-spark} Projekt können damit Spark-Anwendungen ausgeführt werden. In diesem Projekt sind auch weitere Konfigurationen zur Installation von HBase und Solr enthalten.
\end{itemize}

\noindent
Die Dokumentation der Thesis und alle erstellten Diagramme sind unter der Lizenz \textit{\gls{ccbysa}} lizenziert. Der Quellcode der forensischen Analyseplattform und die erstellten Konfigurationen sind unter der Apache Lizenz 2.0 lizenziert. Dadurch ist der Quellcode frei zugänglich und kann von jedem genutzt und verändert werden.\\

\section{Testdatengenerierung}
\label{testdatacreation}
Für den Aufbau einer forensischen Analyseplattform sollen entsprechende Testdaten generiert werden. Hierzu werden zwei unterschiedliche Falldaten erzeugt. Der erste Fall ist ein kleines Datenträgerabbild mit knapp 10 GB Gesamtgröße. Dieses Image soll für lokale Tests genutzt werden, um die entwickelten Implementierungen zur Datenspeicherung und Verarbeitung schnell auf ihre Korrektheit prüfen zu können. Der Datenträger enthält eine Partitionen mit dem Ext4-Dateisystem. Darauf ist ein Ubuntu 16.04 LTS installiert. Zusätzlich sind auch noch einige Testdaten darauf gespeichert.\\

\noindent
Im zweiten Fall wird ein 155 GB großes Image erzeugt. Der Datenträger besteht wiederum aus einer Partition mit einem NTFS-Dateisystem. Darauf ist Windows 10 installiert. Zusätzlich sind größere Datensätze von Bildern und Musik aus dem Internet darauf gespeichert. Die Datensätze werden kostenfrei als Trainingsdaten zum maschinellen Lernen angeboten. Dieses größere Datenträgerabbild kann zur Verarbeitung im Testcluster genutzt werden, um die Skalierbarkeit bei großen Datenmengen zu testen. Beide Datenträgerabbilder werden in Kapitel \ref{sec:performance_analysis} für Leistungstests verwendet. Hierzu befinden sich einige sehr große und auch viele kleine Dateien auf den Datenträgern, um möglichst realitätsnahe Testergebnisse zu erhalten.